name: DeadMan Scraper

on:
  workflow_dispatch:
    inputs:
      target:
        description: 'Target to scrape (reddit, hackernews, github, stackoverflow, or URL)'
        required: true
        default: 'reddit'
      query:
        description: 'Search query'
        required: true
        default: 'free api'
      limit:
        description: 'Max results'
        required: false
        default: '25'

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout DeadManUltimateScraper
        uses: actions/checkout@v4

      - name: Checkout token-optimization
        uses: actions/checkout@v4
        with:
          repository: DeadManOfficial/token-optimization
          path: token-optimization

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install TOR
        run: |
          sudo apt-get update
          sudo apt-get install -y tor
          sudo service tor start
          sleep 10
          curl --socks5-hostname 127.0.0.1:9050 https://check.torproject.org/api/ip || echo "TOR check failed"

      - name: Install dependencies
        run: |
          pip install -e ./token-optimization || true
          pip install curl_cffi "httpx[socks]" beautifulsoup4 pydantic pyyaml stem tldextract fake-useragent requests aiohttp lxml PySocks

      - name: Run Scraper
        env:
          PYTHONPATH: "${{ github.workspace }}/token-optimization/src:${{ github.workspace }}"
        run: |
          python -c "
          import asyncio
          import json
          from deadman_scraper.core.config import Config
          from deadman_scraper.fetch.tor_manager import TORManager

          async def scrape():
              config = Config.from_env()
              tor = TORManager(config.tor)

              if not await tor.start():
                  print('TOR not available')
                  return

              print(f'TOR connected: {tor.proxy_url}')

              from curl_cffi.requests import AsyncSession
              results = []

              target = '${{ github.event.inputs.target }}'
              query = '${{ github.event.inputs.query }}'
              limit = int('${{ github.event.inputs.limit }}')

              async with AsyncSession(impersonate='chrome120', proxy=tor.proxy_url) as session:
                  if target == 'reddit':
                      url = f'https://www.reddit.com/search.json?q={query}&limit={limit}'
                      resp = await session.get(url, timeout=30)
                      if resp.status_code == 200:
                          data = resp.json()
                          results = [{'title': p['data']['title'], 'url': p['data']['url']} for p in data['data']['children']]

                  elif target == 'hackernews':
                      # Use Algolia HN API instead of Firebase
                      url = f'https://hn.algolia.com/api/v1/search?query={query}&hitsPerPage={limit}'
                      resp = await session.get(url, timeout=30)
                      if resp.status_code == 200:
                          data = resp.json()
                          results = [{'title': h['title'], 'url': h.get('url', f\"https://news.ycombinator.com/item?id={h['objectID']}\")} for h in data['hits']]

                  elif target == 'stackoverflow':
                      url = f'https://api.stackexchange.com/2.3/search?order=desc&sort=relevance&intitle={query}&site=stackoverflow&pagesize={limit}'
                      resp = await session.get(url, timeout=30)
                      if resp.status_code == 200:
                          data = resp.json()
                          results = [{'title': q['title'], 'url': q['link']} for q in data['items']]

                  elif target == 'github':
                      url = f'https://api.github.com/search/repositories?q={query}&per_page={limit}'
                      resp = await session.get(url, timeout=30)
                      if resp.status_code == 200:
                          data = resp.json()
                          results = [{'title': r['full_name'], 'url': r['html_url'], 'stars': r['stargazers_count']} for r in data['items']]

                  else:
                      # Direct URL scrape
                      resp = await session.get(target, timeout=30)
                      results = [{'url': target, 'status': resp.status_code, 'length': len(resp.text)}]

              print(f'\\nFound {len(results)} results:')
              for r in results[:10]:
                  print(f\"  - {r.get('title', r.get('url', 'N/A'))[:60]}\")

              # Save results
              with open('scrape_results.json', 'w') as f:
                  json.dump({'target': target, 'query': query, 'count': len(results), 'results': results}, f, indent=2)

          asyncio.run(scrape())
          "

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results
          path: scrape_results.json

      - name: Summary
        run: |
          echo "### DeadMan Scraper Results" >> "$GITHUB_STEP_SUMMARY"
          echo "Target: ${{ github.event.inputs.target }}" >> "$GITHUB_STEP_SUMMARY"
          echo "Query: ${{ github.event.inputs.query }}" >> "$GITHUB_STEP_SUMMARY"
          if [ -f scrape_results.json ]; then
            count=$(jq '.count' scrape_results.json)
            echo "Results: $count" >> "$GITHUB_STEP_SUMMARY"
          fi

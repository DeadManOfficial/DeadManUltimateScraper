name: DeadMan Scraper
# Integrated 8-method bypass chain

on:
  workflow_dispatch:
    inputs:
      target:
        description: 'Target (reddit, hackernews, github, stackoverflow, or any URL)'
        required: true
        default: 'reddit'
      query:
        description: 'Search query (for API targets)'
        required: false
        default: 'free api'
      limit:
        description: 'Max results'
        required: false
        default: '25'
      extract:
        description: 'Extract mode (all, text, links, json, prompts, or CSS selector)'
        required: false
        default: 'all'
      render:
        description: 'Force browser rendering for JavaScript SPAs'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      use_cookies:
        description: 'Use stored session cookies for authentication'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    services:
      flaresolverr:
        image: ghcr.io/flaresolverr/flaresolverr:v3.3.21
        ports:
          - 8191:8191
        env:
          LOG_LEVEL: info
          TZ: America/New_York

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Chrome
        run: |
          wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo dpkg -i google-chrome-stable_current_amd64.deb || sudo apt-get -f install -y
          sudo apt-get install -y xvfb

      - name: Install TOR
        run: |
          sudo apt-get update
          sudo apt-get install -y tor
          sudo service tor start
          sleep 5

      - name: Install dependencies
        run: |
          pip install \
            curl_cffi \
            cloudscraper \
            playwright \
            playwright-stealth \
            undetected-chromedriver \
            nodriver \
            botasaurus \
            httpx[socks] \
            beautifulsoup4 \
            lxml \
            pydantic \
            requests \
            aiohttp

          playwright install chromium
          playwright install-deps chromium

      - name: Run Scraper
        env:
          DISPLAY: ":99"
          INPUT_TARGET: ${{ github.event.inputs.target }}
          INPUT_QUERY: ${{ github.event.inputs.query }}
          INPUT_LIMIT: ${{ github.event.inputs.limit }}
          INPUT_EXTRACT: ${{ github.event.inputs.extract }}
          INPUT_RENDER: ${{ github.event.inputs.render }}
          INPUT_USE_COOKIES: ${{ github.event.inputs.use_cookies }}
        run: |
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 2
          python .github/scripts/scrape.py

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results
          path: |
            scrape_results.json
            raw_response.html

      - name: Summary
        run: |
          echo "### DeadMan Scraper Results" >> "$GITHUB_STEP_SUMMARY"
          echo "Target: ${{ github.event.inputs.target }}" >> "$GITHUB_STEP_SUMMARY"
          if [ -f scrape_results.json ]; then
            success=$(jq '.success' scrape_results.json)
            method=$(jq -r '.method_used' scrape_results.json)
            count=$(jq '.count' scrape_results.json)
            echo "Success: $success" >> "$GITHUB_STEP_SUMMARY"
            echo "Method: $method" >> "$GITHUB_STEP_SUMMARY"
            echo "Results: $count" >> "$GITHUB_STEP_SUMMARY"
          fi
